{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje Reforzado\n",
    "## El laberinto del muffintauro\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Programe el ambiente que simule el laberinto del muffintaurio, debe construir una función step(state, action) que reciba un estado y una acción, y que devuelva el estado siguiente y la recompensa. nota: este ambiente es determinístico, por lo que la probabilidad de transición a un determinado s' es 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El estado siguiente es el nodo 3, con 6 muffins y su recompensa es 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nodo = 1  # Elegir el nodo de partida\n",
    "muffins_max = 10\n",
    "muffins = 6  # Elegir la cantidad de muffins que lleva consigo\n",
    "state = (nodo, muffins)\n",
    "acciones_posibles = [(0, 1), (0, 2), (1, 0), (1, 3), (2, 0), (2, 3)]\n",
    "destino = 3 #Elegir el nodo de destino\n",
    "action = (nodo, destino) \n",
    "\n",
    "# Dependiendo el nodo al que vaya calcula el reward o costo de este\n",
    "def rewards(state, action): \n",
    "    if state[1] == 0:\n",
    "        return -42\n",
    "    if action[1] == 1:\n",
    "        return 0\n",
    "    if action[1] == 2:\n",
    "        return -1\n",
    "    if action[1] == 3:\n",
    "        return 6 - abs(state[1] - 6)\n",
    "\n",
    "# Actúa en los casos en los que se quiten u obtengan muffins en el recorrido\n",
    "def manejo_de_muffins(action):\n",
    "    if action[1] == 1:\n",
    "        return -2\n",
    "    elif action[1] == 2:\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# Función step\n",
    "def step(state, action):\n",
    "    if action in acciones_posibles and state[0] == action[0]:\n",
    "        r_destino = rewards(state, action)\n",
    "        nuevo_muffins = state[1] + manejo_de_muffins(action)\n",
    "        new_state = (action[1], max(0, min(muffins_max, nuevo_muffins))) \n",
    "        print(f\"El estado siguiente es el nodo {new_state[0]}, con {new_state[1]} muffins y su recompensa es {r_destino}\")\n",
    "    else:\n",
    "        print(\"Acción no válida\")\n",
    "\n",
    "# Ejecuta la función\n",
    "step(state, action)\n",
    "\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Encuentre la política optima usando Value Iteration, para esto lea el capítulo 4.4. del libro de Sutton y Barto e implemente el algoritmo. Use una tolerancia tol = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La política óptima del nodo 0 llevando 0 muffins es ir al nodo 2\n",
      "La política óptima del nodo 0 llevando 1 muffins es ir al nodo 2\n",
      "La política óptima del nodo 0 llevando 2 muffins es ir al nodo 2\n",
      "La política óptima del nodo 0 llevando 3 muffins es ir al nodo 2\n",
      "La política óptima del nodo 0 llevando 4 muffins es ir al nodo 2\n",
      "La política óptima del nodo 0 llevando 5 muffins es ir al nodo 2\n",
      "La política óptima del nodo 0 llevando 6 muffins es ir al nodo 1\n",
      "La política óptima del nodo 0 llevando 7 muffins es ir al nodo 1\n",
      "La política óptima del nodo 0 llevando 8 muffins es ir al nodo 1\n",
      "La política óptima del nodo 0 llevando 9 muffins es ir al nodo 1\n",
      "La política óptima del nodo 0 llevando 10 muffins es ir al nodo 1\n",
      "La política óptima del nodo 1 llevando 0 muffins es ir al nodo 3\n",
      "La política óptima del nodo 1 llevando 1 muffins es ir al nodo 0\n",
      "La política óptima del nodo 1 llevando 2 muffins es ir al nodo 0\n",
      "La política óptima del nodo 1 llevando 3 muffins es ir al nodo 0\n",
      "La política óptima del nodo 1 llevando 4 muffins es ir al nodo 3\n",
      "La política óptima del nodo 1 llevando 5 muffins es ir al nodo 3\n",
      "La política óptima del nodo 1 llevando 6 muffins es ir al nodo 3\n",
      "La política óptima del nodo 1 llevando 7 muffins es ir al nodo 3\n",
      "La política óptima del nodo 1 llevando 8 muffins es ir al nodo 0\n",
      "La política óptima del nodo 1 llevando 9 muffins es ir al nodo 0\n",
      "La política óptima del nodo 1 llevando 10 muffins es ir al nodo 0\n",
      "La política óptima del nodo 2 llevando 0 muffins es ir al nodo 3\n",
      "La política óptima del nodo 2 llevando 1 muffins es ir al nodo 0\n",
      "La política óptima del nodo 2 llevando 2 muffins es ir al nodo 0\n",
      "La política óptima del nodo 2 llevando 3 muffins es ir al nodo 0\n",
      "La política óptima del nodo 2 llevando 4 muffins es ir al nodo 3\n",
      "La política óptima del nodo 2 llevando 5 muffins es ir al nodo 3\n",
      "La política óptima del nodo 2 llevando 6 muffins es ir al nodo 3\n",
      "La política óptima del nodo 2 llevando 7 muffins es ir al nodo 3\n",
      "La política óptima del nodo 2 llevando 8 muffins es ir al nodo 0\n",
      "La política óptima del nodo 2 llevando 9 muffins es ir al nodo 0\n",
      "La política óptima del nodo 2 llevando 10 muffins es ir al nodo 0\n"
     ]
    }
   ],
   "source": [
    "# La estructura del laberinto\n",
    "nodos = [0, 1, 2, 3]\n",
    "acciones = [(0, 1), (0, 2), (1, 0), (1, 3), (2, 0), (2, 3)]\n",
    "gamma = 0.9  # Factor de descuento\n",
    "tol = 0.1  # Umbral para la convergencia\n",
    "muffins_max = 10  # Tope de cantidad de muffins\n",
    "\n",
    "# Inicializar el grafo como un diccionario de listas\n",
    "laberinto = {\n",
    "    0: [1, 2],\n",
    "    1: [0, 3],\n",
    "    2: [0, 3],\n",
    "    3: []\n",
    "}\n",
    "\n",
    "# Función de recompensa\n",
    "def calcular_recompensa(state, destino):\n",
    "    nodo, muffins = state\n",
    "    if muffins <= 0:\n",
    "        return -42  # Penalización por quedarse sin muffins\n",
    "    if destino == 2:\n",
    "        return -1  # Penalización por pasar por el nodo 2\n",
    "    if destino == 3:\n",
    "        return 6 - abs(muffins - 6)  # Recompensa al llegar al nodo final\n",
    "    return 0  # Recompensa por otros movimientos\n",
    "\n",
    "# Función para calcular los muffins después de una acción\n",
    "def calcular_muffins(state, destino):\n",
    "    nodo, muffins = state\n",
    "    if destino == 1:\n",
    "        muffins -= 2\n",
    "    elif destino == 2:\n",
    "        muffins += 2\n",
    "    return max(0, min(muffins_max, muffins))  # Asegurarse de que los muffins estén en el rango permitido\n",
    "\n",
    "# Inicializar los valores de los estados a cero\n",
    "valores = {(nodo, muffins): 0 for nodo in nodos for muffins in range(muffins_max + 1)}\n",
    "\n",
    "# Inicializar las políticas\n",
    "politicas = {(nodo, muffins): None for nodo in nodos for muffins in range(muffins_max + 1)}\n",
    "\n",
    "# Función de iteración de valores\n",
    "def iteracion_de_valores(laberinto, gamma, tol):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        nuevos_valores = valores.copy()\n",
    "        for nodo in nodos:\n",
    "            for muffins in range(muffins_max + 1):\n",
    "                state = (nodo, muffins)\n",
    "                if nodo == 3:\n",
    "                    continue  # No hay necesidad de actualizar el nodo final\n",
    "\n",
    "                max_valor = float('-inf')\n",
    "                mejor_accion = None\n",
    "                for destino in laberinto[nodo]:\n",
    "                    recompensa = calcular_recompensa(state, destino)\n",
    "                    nuevo_muffins = calcular_muffins(state, destino)\n",
    "\n",
    "                    # Valor descontado del siguiente estado\n",
    "                    nuevo_estado = (destino, nuevo_muffins)\n",
    "                    valor_estado_siguiente = valores[nuevo_estado]\n",
    "                    valor_actual = recompensa + gamma * valor_estado_siguiente\n",
    "\n",
    "                    if valor_actual > max_valor:\n",
    "                        max_valor = valor_actual\n",
    "                        mejor_accion = destino\n",
    "\n",
    "                nuevos_valores[state] = max_valor\n",
    "                politicas[state] = mejor_accion\n",
    "                delta = max(delta, abs(valores[state] - nuevos_valores[state]))\n",
    "\n",
    "        valores.update(nuevos_valores)\n",
    "        if delta < tol:\n",
    "            break\n",
    "\n",
    "    return valores, politicas\n",
    "\n",
    "# Ejecutar iteración de valores\n",
    "valores_optimos, politicas_optimas = iteracion_de_valores(laberinto, gamma, tol)\n",
    "\n",
    "# Imprimir las políticas óptimas de los estados, excluyendo el nodo 3\n",
    "for estado, accion in sorted(politicas_optimas.items()):\n",
    "    if estado[0] == 3:\n",
    "        continue  # No imprimir políticas para el nodo 3, ya que terminó el recorrido\n",
    "    print(f\"La política óptima del nodo {estado[0]} llevando {estado[1]} muffins es ir al nodo {accion}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Imprima una tabla de nodo vs cantidad de muffins que muestre el valor de cada estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cada estado se compone por el nodo actual y la cantidad de muffins. Los valores de cada estado son: \n",
      "Muffins      0       1     2     3    4    5    6    7     8     9       10\n",
      "Nodo                                                                       \n",
      "0       -39.894  1.8350  2.60  3.50  4.4  3.5  3.6  4.5  5.40  4.50  4.3740\n",
      "1       -42.000  1.6515  2.34  3.15  4.0  5.0  6.0  5.0  4.86  4.05  3.9366\n",
      "2       -42.000  1.6515  2.34  3.15  4.0  5.0  6.0  5.0  4.86  4.05  3.9366\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# La estructura del laberinto\n",
    "nodos = [0, 1, 2, 3]\n",
    "acciones = [(0, 1), (0, 2), (1, 0), (1, 3), (2, 0), (2, 3)]\n",
    "gamma = 0.9  # Factor de descuento\n",
    "tol = 0.1  # Umbral para la convergencia\n",
    "muffins_max = 10  # Topde de cantidad de muffins\n",
    "\n",
    "# Inicializar el grafo como un diccionario de listas\n",
    "laberinto = {\n",
    "    0: [1, 2],\n",
    "    1: [0, 3],\n",
    "    2: [0, 3],\n",
    "    3: []\n",
    "}\n",
    "\n",
    "# Función de recompensa\n",
    "def calcular_recompensa(state, destino):\n",
    "    nodo, muffins = state\n",
    "    if muffins <= 0:\n",
    "        return -42  # Penalización por quedarse sin muffins\n",
    "    if destino == 2:\n",
    "        return -1  # Penalización por pasar por el nodo 2\n",
    "    if destino == 3:\n",
    "        return 6 - abs(muffins - 6)  # Recompensa al llegar al nodo final\n",
    "    return 0  # Recompensa por otros movimientos\n",
    "\n",
    "# Función para calcular los muffins después de una acción\n",
    "def calcular_muffins(state, destino):\n",
    "    nodo, muffins = state\n",
    "    if destino == 1:\n",
    "        muffins -= 2\n",
    "    elif destino == 2:\n",
    "        muffins += 2\n",
    "    return max(0, min(muffins_max, muffins))  # Asegurarse de que los muffins estén en el rango permitido\n",
    "\n",
    "# Inicializar los valores de los estados a cero\n",
    "valores = {(nodo, muffins): 0 for nodo in nodos for muffins in range(muffins_max + 1)}\n",
    "\n",
    "# Función de iteración de valores\n",
    "def iteracion_de_valores(laberinto, gamma, tol):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        nuevos_valores = valores.copy()\n",
    "        for nodo in nodos:\n",
    "            for muffins in range(muffins_max + 1):\n",
    "                state = (nodo, muffins)\n",
    "                if nodo == 3:\n",
    "                    continue  # No hay necesidad de actualizar el nodo final\n",
    "\n",
    "                max_valor = float('-inf')\n",
    "                for destino in laberinto[nodo]:\n",
    "                    recompensa = calcular_recompensa(state, destino)\n",
    "                    nuevo_muffins = calcular_muffins(state, destino)\n",
    "\n",
    "                    # Valor descontado del siguiente estado\n",
    "                    nuevo_estado = (destino, nuevo_muffins)\n",
    "                    valor_estado_siguiente = valores[nuevo_estado]\n",
    "                    valor_actual = recompensa + gamma * valor_estado_siguiente\n",
    "                    # Como es determinístico no se lo multiplica por la probabilidad (esta es 1)\n",
    "\n",
    "                    max_valor = max(max_valor, valor_actual)\n",
    "\n",
    "                nuevos_valores[state] = max_valor\n",
    "                delta = max(delta, abs(valores[state] - nuevos_valores[state]))\n",
    "\n",
    "        valores.update(nuevos_valores)\n",
    "        if delta < tol:\n",
    "            break\n",
    "\n",
    "    return valores\n",
    "\n",
    "# Ejecutar iteración de valores\n",
    "valores_optimos = iteracion_de_valores(laberinto, gamma, tol)\n",
    "\n",
    "# Crear una tabla con pandas\n",
    "data = []\n",
    "for (nodo, muffins), valor in valores_optimos.items():\n",
    "    data.append([nodo, muffins, valor])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"Nodo\", \"Muffins\", \"Valor\"])\n",
    "\n",
    "df = df[df[\"Nodo\"] != 3] # Evito imprimir la fila nula asociada al nodo terminal\n",
    "\n",
    "pivot_table = df.pivot(index=\"Nodo\", columns=\"Muffins\", values=\"Valor\")\n",
    "\n",
    "# Imprimir la tabla\n",
    "print(\"Cada estado se compone por el nodo actual y la cantidad de muffins. Los valores de cada estado son: \")\n",
    "print(pivot_table)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Imprima una tabla de nodo vs cantidad de muffins que muestre la política optima para cada estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La política óptima de cada estado no terminal es:\n",
      "Muffins  0   1   2   3   4   5   6   7   8   9   10\n",
      "Nodo                                               \n",
      "0         2   2   2   2   2   2   1   1   1   1   1\n",
      "1         3   0   0   0   3   3   3   3   0   0   0\n",
      "2         3   0   0   0   3   3   3   3   0   0   0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# La estructura del laberinto\n",
    "nodos = [0, 1, 2, 3]\n",
    "acciones = [(0, 1), (0, 2), (1, 0), (1, 3), (2, 0), (2, 3)]\n",
    "gamma = 0.9  # Factor de descuento\n",
    "tol = 0.1  # Umbral para la convergencia\n",
    "muffins_max = 10  # Tope de cantidad de muffins\n",
    "\n",
    "# Inicializar el grafo como un diccionario de listas\n",
    "laberinto = {\n",
    "    0: [1, 2],\n",
    "    1: [0, 3],\n",
    "    2: [0, 3],\n",
    "    3: []\n",
    "}\n",
    "\n",
    "# Función de recompensa\n",
    "def calcular_recompensa(state, destino):\n",
    "    nodo, muffins = state\n",
    "    if muffins <= 0:\n",
    "        return -42  # Penalización por quedarse sin muffins\n",
    "    if destino == 2:\n",
    "        return -1  # Penalización por pasar por el nodo 2\n",
    "    if destino == 3:\n",
    "        return 6 - abs(muffins - 6)  # Recompensa al llegar al nodo final\n",
    "    return 0  # Recompensa por otros movimientos\n",
    "\n",
    "# Función para calcular los muffins después de una acción\n",
    "def calcular_muffins(state, destino):\n",
    "    nodo, muffins = state\n",
    "    if destino == 1:\n",
    "        muffins -= 2\n",
    "    elif destino == 2:\n",
    "        muffins += 2\n",
    "    return max(0, min(muffins_max, muffins))  # Asegurarse de que los muffins estén en el rango permitido\n",
    "\n",
    "# Inicializar los valores de los estados a cero\n",
    "valores = {(nodo, muffins): 0 for nodo in nodos for muffins in range(muffins_max + 1)}\n",
    "\n",
    "# Inicializar las políticas\n",
    "politicas = {(nodo, muffins): None for nodo in nodos for muffins in range(muffins_max + 1)}\n",
    "\n",
    "# Función de iteración de valores\n",
    "def iteracion_de_valores(laberinto, gamma, tol):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        nuevos_valores = valores.copy()\n",
    "        for nodo in nodos:\n",
    "            for muffins in range(muffins_max + 1):\n",
    "                state = (nodo, muffins)\n",
    "                if nodo == 3:\n",
    "                    continue  # No hay necesidad de actualizar el nodo final\n",
    "\n",
    "                max_valor = float('-inf')\n",
    "                mejor_accion = None\n",
    "                for destino in laberinto[nodo]:\n",
    "                    recompensa = calcular_recompensa(state, destino)\n",
    "                    nuevo_muffins = calcular_muffins(state, destino)\n",
    "\n",
    "                    # Valor descontado del siguiente estado\n",
    "                    nuevo_estado = (destino, nuevo_muffins)\n",
    "                    valor_estado_siguiente = valores[nuevo_estado]\n",
    "                    valor_actual = recompensa + gamma * valor_estado_siguiente\n",
    "\n",
    "                    if valor_actual > max_valor:\n",
    "                        max_valor = valor_actual\n",
    "                        mejor_accion = destino\n",
    "\n",
    "                nuevos_valores[state] = max_valor\n",
    "                politicas[state] = mejor_accion\n",
    "                delta = max(delta, abs(valores[state] - nuevos_valores[state]))\n",
    "\n",
    "        valores.update(nuevos_valores)\n",
    "        if delta < tol:\n",
    "            break\n",
    "\n",
    "    return valores, politicas\n",
    "\n",
    "# Ejecutar iteración de valores\n",
    "valores_optimos, politicas_optimas = iteracion_de_valores(laberinto, gamma, tol)\n",
    "\n",
    "# Crear una tabla con pandas\n",
    "data = []\n",
    "for (nodo, muffins), accion in politicas_optimas.items():\n",
    "    if accion is not None:\n",
    "        data.append([nodo, muffins, int(accion)])\n",
    "\n",
    "# Crear un DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Nodo\", \"Muffins\", \"Política Óptima\"])\n",
    "\n",
    "# Filtrar el DataFrame para eliminar la fila con el nodo 3 porque ahí termina el recorrido\n",
    "df = df[df[\"Nodo\"] != 3]\n",
    "\n",
    "# Pivotar la tabla\n",
    "pivot_table = df.pivot(index=\"Nodo\", columns=\"Muffins\", values=\"Política Óptima\")\n",
    "\n",
    "# Imprimir la tabla\n",
    "print(\"La política óptima de cada estado no terminal es:\")\n",
    "print(pivot_table)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Haga un análisis cualitativo de la política optima. ¿Tiene sentido?¿Es intuitiva?¿Es la que esperaba?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mayor recompensa que se puede obtener en el recorrido es la que se consigue cuando se llega a la casa de la abuela con una cierta cantidad de muffins. El mejor caso es llegar con 6 muffins, ya que ahi la recompensa sera de 6 monedas. Luego se pueden obtener 5 monedas si se llega con 5 o 7 muffins, 4 monedas si se entregan 4 o 8 muffins y asi sucesivamente. Mientras más se aleja el número de muffins del número 6, menor es la recompensa al terminar el recorrido. Es por esto que la política óptima de estar en el nodo 1 o 2 con 4, 5, 6 y 7 muffins es terminar el recorrido, ya que ahí la recompensa va a ser alta. Pero, en el caso de estar en alguno de estos dos nodos y llevar 8 muffins, aún si la recompensa de llevar 8 muffins es igual a la de 4 (y en esta ocación la política óptima es terminar el recorrido), en este caso existe una mejor opción que ir al nodo 3. Esto se debe a que si se vuelve al nodo 0 para despues ir al nodo 1 y perder 2 muffins, como indican sus políticas óptimas, se puede terminar el recorrido con 6 muffins y sin ningún costo adicional, obtener la mayor recompensa posible. Esto tiene mucho sentido, ya que sin ninguna recompensa negativa solo se pueden perden muffins en el camino, tener 8 muffins en el nodo 1 o 2 es mejor que tener 4 en estos, aún si su recompensa por terminar en ese instante sea la misma. La política óptima hace exactamente esto, para cuidar las recompensas de cada estado. \n",
    "\n",
    "Por otro lado, cuando el laberinto se empieza con menos de 6 muffins, la política óptima indica que es mejor ir para el nodo 2, ya que en este es posible aumentar la cantidad de muffins y terminar el recorrido con una cantidad más cercana a 6 muffins. Aún si con el paso por el nodo 2 no se llega a un número cercano al 6, la política óptima indica que se debe volver al nodo inicial para volver a pasar por el nodo 2 y así seguir aum,entando la cantidad de muffins. Esto tiene mucho sentido, porque aún si aumentando la cantidad de muffins pierden una moneda cada vez, este costo sigue siendo menor a la recompensa por terminar con 5, 6 o 7 muffins. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Mirando la tabla de valores, ¿Con cuantos muffins conviene empezar el dia?¿Depende del factor de descuento?\n",
    "\n",
    "Observando la tabla de valores, conviene arrancar el día con 8 muffins, y de ahí seguir conm la política óptima. Esta consiste en ir al nodo 1 y sin ningún costo perder 2 muffins para luego terminar el recorrido con 6 muffins y una recompensa de 6 monedas. El valor de arrancar el recorrido con 8 muffins es mucho mayor a cualquier otra cantidad. Aún probando con distintos valores para $\\gamma$ el mayor valor al estar en el nodo 1 sigue perteneciendo al que posee 8 muffins.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
